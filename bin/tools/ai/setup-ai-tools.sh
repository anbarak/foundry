#!/usr/bin/env bash
set -euo pipefail

# Load brew path helpers if present (macOS/Linuxbrew)
# so `ollama` can be resolved via PATH cross-platform.
if [[ -f "$HOME/bin/lib/ensure-brew-path.sh" ]]; then
  # shellcheck disable=SC1091
  source "$HOME/bin/lib/ensure-brew-path.sh"
fi

# Paths
AI_DIR="$HOME/bin/tools/ai"
LOG_DIR="$HOME/logs"

MODE="${1:-interactive}"

# Determine if interactive session (user terminal) or not (LaunchAgent, cron, etc)
IS_INTERACTIVE=false
[[ -t 0 && "$MODE" != "non-interactive" ]] && IS_INTERACTIVE=true

# Prompt user before proceeding
if [[ "$MODE" == "non-interactive" ]]; then
  enable_ai="y"
elif read -rp \
  "ğŸ¤– Do you want to enable AI tooling (Ollama, local models, Aider)? [y/N]: " enable_ai \
  && [[ "$enable_ai" =~ ^[Yy]$ ]]; then
  :
else
  echo "ğŸš« Skipped AI tooling setup"
  exit 0
fi

if [[ "$enable_ai" =~ ^[Yy]$ ]]; then
  echo "ğŸ”§ Setting up AI tooling..."

  # Create folders
  mkdir -p "$AI_DIR" "$LOG_DIR"

  # Ensure ollama exists in PATH
  if ! command -v ollama >/dev/null 2>&1; then
    echo "âŒ ollama not found in PATH"
    echo "   Make sure Homebrew/Linuxbrew is installed and ensure-brew-path.sh is sourced."
    exit 1
  fi

  # Start Ollama server if not running
  if ! pgrep -f "ollama serve" >/dev/null 2>&1; then
    echo "ğŸš€ Starting Ollama server..."
    nohup ollama serve > "$LOG_DIR/ollama.log" 2>&1 &

    # Wait for Ollama to be ready (max 30 seconds)
    echo "â³ Waiting for Ollama server to start..."
    i=1
    while [[ "$i" -le 30 ]]; do
      if ollama list >/dev/null 2>&1; then
        echo "âœ… Ollama server is ready"
        break
      fi
      sleep 1
      i=$((i + 1))
    done

    # Final check
    if ! ollama list >/dev/null 2>&1; then
      echo "âŒ Ollama server failed to start. Check logs at $LOG_DIR/ollama.log"
      exit 1
    fi
  else
    echo "âœ… Ollama server already running"
  fi

  # Determine which models to pull
  DEFAULT_MODEL_FILE="$HOME/.local/state/foundry/ollama-default-model"
  DEFAULT_MODEL=$(cat "$DEFAULT_MODEL_FILE" 2>/dev/null || echo llama3.2)

  echo "ğŸ“¦ Using default model only: $DEFAULT_MODEL"
  if ! ollama show "$DEFAULT_MODEL" >/dev/null 2>&1; then
    echo "â¬‡ï¸  Pulling model: $DEFAULT_MODEL"
    ollama pull "$DEFAULT_MODEL"
  else
    echo "âœ… Model already pulled: $DEFAULT_MODEL"
  fi

  # Install LaunchAgent to auto-start Ollama at login
  INSTALLER="$AI_DIR/install-ai-tools.sh"
  if [[ -x "$INSTALLER" ]]; then
    "$INSTALLER"
  fi

  if [[ "$IS_INTERACTIVE" == true ]]; then
    echo "ğŸ“¦ Ensuring Colima is running to pull aider Docker image..."

    COLIMA_STARTED=false
    if ! colima status | grep -q "Status: Running"; then
      echo "ğŸš€ Starting Colima temporarily..."
      colima start
      COLIMA_STARTED=true
    else
      echo "âœ… Colima already running"
    fi

    echo "ğŸ“¦ Pulling aider-full Docker image..."
    docker pull paulgauthier/aider-full

    if [[ "$COLIMA_STARTED" == true ]]; then
      echo "ğŸ›‘ Stopping Colima (was started just for this task)..."
      colima stop
    fi
  else
    echo "â­ï¸ Skipping aider Docker pull in non-interactive mode"
  fi

else
  echo "ğŸš« Skipped AI tooling setup"
fi
